"""
Unified Phase 2: Frame Extraction + CLIP Embedding

Optimized single-file solution that replaces sample_frames.py + embed_frames.py
Reduces 544 LOC to ~120 LOC while maintaining identical interface compatibility.
"""

import json
import math
from pathlib import Path
from typing import List, Dict, Any
from dataclasses import dataclass

import ffmpeg
import numpy as np
from PIL import Image
import torch
import open_clip
from tqdm import tqdm


@dataclass
class FrameData:
    """Unified frame data structure combining metadata and embedding."""
    video_id: str
    timestamp: float  
    frame_path: str
    embedding: np.ndarray = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary format expected by database models."""
        return {
            'video_id': self.video_id,
            'start': self.timestamp,
            'end': self.timestamp + 10.0,  # Default 10s interval for compatibility
            'frame_path': self.frame_path,
            'timestamp': self.timestamp,
            'embedding': self.embedding.tolist() if self.embedding is not None else None
        }


class FrameProcessor:
    """
    Unified frame extraction and embedding processor.
    
    Replaces FrameSampler + FrameEmbedder with a single streamlined class
    that maintains identical interface compatibility.
    """
    
    def __init__(self, frames_dir: str, interval: int = 10):
        """
        Initialize processor.
        
        Args:
            frames_dir: Directory to store extracted frames
            interval: Time interval between frames in seconds
        """
        self.frames_dir = Path(frames_dir)
        self.frames_dir.mkdir(parents=True, exist_ok=True)
        self.interval = interval
        self.model = None
        self.preprocess = None
        self.device = None
        
    def _load_clip_model(self, model_name: str = "ViT-B-32", pretrained: str = "openai"):
        """Load CLIP model once when needed."""
        if self.model is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"üîß Initializing CLIP {model_name} on {self.device}")
            
            self.model, _, self.preprocess = open_clip.create_model_and_transforms(
                model_name, pretrained=pretrained, device=self.device
            )
            self.model.eval()
            
            # Get embedding dimension
            with torch.no_grad():
                dummy_image = torch.randn(1, 3, 224, 224).to(self.device)
                dummy_embedding = self.model.encode_image(dummy_image)
                embedding_dim = dummy_embedding.shape[-1]
            
            print(f"‚úÖ CLIP model loaded successfully")
            print(f"   Model: {model_name} ({pretrained})")
            print(f"   Embedding dimension: {embedding_dim}")
    
    def sample_frames(self, video_path: str, video_id: str) -> List[Any]:
        """
        Extract frames from video at regular intervals.
        
        Maintains identical interface to original FrameSampler.sample_frames()
        Returns FrameMetadata-compatible objects for backward compatibility.
        """
        if not Path(video_path).exists():
            raise FileNotFoundError(f"Video file not found: {video_path}")
        
        # Get video duration
        duration = float(ffmpeg.probe(video_path)['streams'][0]['duration'])
        print(f"Video duration: {duration:.2f} seconds")
        
        # Calculate timestamps
        timestamps = []
        current_time = 0.0
        while current_time < duration:
            timestamps.append(current_time)
            current_time += self.interval
        
        print(f"Extracting {len(timestamps)} frames at {self.interval}s intervals...")
        
        # Extract frames and create metadata
        frame_metadata = []
        for timestamp in tqdm(timestamps, desc="Extracting frames"):
            frame_filename = f"{video_id}_{int(timestamp)}.jpg"
            frame_path = self.frames_dir / frame_filename
            
            # Extract frame using ffmpeg
            ffmpeg.input(video_path, ss=timestamp).output(
                str(frame_path), vframes=1, loglevel='quiet'
            ).overwrite_output().run()
            
            # Create FrameMetadata-compatible object
            metadata = type('FrameMetadata', (), {
                'video_id': video_id,
                'start': timestamp,
                'end': min(timestamp + self.interval, duration),
                'frame_path': str(frame_path),
                'timestamp': timestamp
            })()
            
            frame_metadata.append(metadata)
        
        print(f"Successfully extracted {len(frame_metadata)} frames to {self.frames_dir}")
        return frame_metadata
    
    def embed_frames(self, frame_metadata: List[Any], batch_size: int = 64) -> List[Any]:
        """
        Embed frames using CLIP.
        
        Maintains identical interface to original FrameEmbedder.embed_frames()
        Returns FrameEmbedding-compatible objects for backward compatibility.
        """
        if not frame_metadata:
            return []
        
        print(f"üñºÔ∏è Embedding {len(frame_metadata)} frames (batch_size={batch_size})")
        
        # Load CLIP model
        self._load_clip_model()
        
        embeddings = []
        
        # Process in batches
        with torch.no_grad():
            for i in tqdm(range(0, len(frame_metadata), batch_size), desc="Embedding frames"):
                batch_metadata = frame_metadata[i:i + batch_size]
                
                # Load and preprocess images
                images = []
                for frame_meta in batch_metadata:
                    img = Image.open(frame_meta.frame_path).convert('RGB')
                    images.append(self.preprocess(img))
                
                if images:
                    # Generate embeddings
                    batch_tensor = torch.stack(images).to(self.device)
                    batch_embeddings = self.model.encode_image(batch_tensor).cpu().numpy()
                    
                    # Normalize embeddings
                    batch_embeddings = batch_embeddings / np.linalg.norm(batch_embeddings, axis=1, keepdims=True)
                    
                    # Create FrameEmbedding-compatible objects
                    for j, (embedding, frame_meta) in enumerate(zip(batch_embeddings, batch_metadata)):
                        # Create a proper data structure to avoid closure issues
                        frame_data = {
                            'video_id': frame_meta.video_id,
                            'start': frame_meta.start,
                            'end': frame_meta.end,
                            'frame_path': frame_meta.frame_path,
                            'timestamp': frame_meta.timestamp,
                            'embedding': embedding.tolist()
                        }
                        
                        # Create object with proper to_dict method using default parameter trick
                        def make_to_dict(data):
                            return lambda self=None: data
                        
                        frame_embedding = type('FrameEmbedding', (), {
                            'video_id': frame_data['video_id'],
                            'start': frame_data['start'],
                            'end': frame_data['end'],
                            'frame_path': frame_data['frame_path'],
                            'timestamp': frame_data['timestamp'],
                            'embedding': embedding,
                            'to_dict': make_to_dict(frame_data)
                        })()
                        
                        embeddings.append(frame_embedding)
        
        print(f"‚úÖ Successfully embedded {len(embeddings)} frames")
        return embeddings


# Backward compatibility aliases
class FrameSampler:
    """Backward compatibility wrapper for FrameProcessor."""
    
    def __init__(self, frames_dir: str = "data/frames", interval: int = 10):
        self.processor = FrameProcessor(frames_dir, interval)
    
    def sample_frames(self, video_path: str, video_id: str):
        return self.processor.sample_frames(video_path, video_id)


class FrameEmbedder:
    """Backward compatibility wrapper for FrameProcessor."""
    
    def __init__(self, model_name: str = "ViT-B-32", pretrained: str = "openai"):
        self.model_name = model_name
        self.pretrained = pretrained
        # Create a dummy processor - actual model loading happens in embed_frames
        self.processor = FrameProcessor("temp")
    
    def embed_frames(self, frame_metadata: List[Any], batch_size: int = 64):
        # Load model with specified parameters
        self.processor._load_clip_model(self.model_name, self.pretrained)
        return self.processor.embed_frames(frame_metadata, batch_size)