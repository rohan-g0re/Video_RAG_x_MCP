# Multimodal Videoâ€‘Only RAG â€‘ Detailed Development Plan

---

## ðŸ“‘â€¯Project Scope

**Goal:** Build a fullyâ€‘local, videoâ€‘only Retrievalâ€‘Augmented Generation (RAG) pipeline that answers naturalâ€‘language queries and returns (a) a textual answer citing timestamps and (b) clipped video segments corresponding to those timestamps.

*All code in Python, managed in virtualenv.*

---

## ðŸ—‚Â Directory Layout (Monorepo)

```
repo_root/
 â”œâ”€ data/                # raw + processed
 â”‚   â”œâ”€ raw_videos/
 â”‚   â”œâ”€ frames/          # extracted .jpg
 â”‚   â””â”€ clips/           # returned snippets
 â”œâ”€ src/
 â”‚   â”œâ”€ phase1_audio/
 â”‚   â”œâ”€ phase2_visual/
 â”‚   â”œâ”€ phase3_db/
 â”‚   â”œâ”€ phase4_retriever/
 â”‚   â”œâ”€ phase5_generation/
 â”‚   â””â”€ phase6_clipper/
 â”œâ”€ tests/
 â”œâ”€ conf/                # .env, model paths, settings.yaml
 â””â”€ README.md
```

Each subâ€‘package has its **own ****\`\`**** entry point** so phases can be run as standalone modules.

---

## PhaseÂ 1Â â€“ Audio Processing & Embedding

### 1â€‘A  |Â Audio Extraction & Transcription

| Item          | Detail                                                    |
| ------------- | --------------------------------------------------------- |
| **Tool**      | OpenAI Whisper (CPU, `medium` model) via `openai-whisper` |
| **Script**    | `src/phase1_audio/extract_transcribe.py`                  |
| **Algorithm** | 1. `ffmpeg -i video.mp4 -vn -acodec pcm_s16le temp.wav`   |

2. Run Whisper \*\*with \*\*\`\` to obtain JSON with perâ€‘word timings. | | **Output** | `data/transcripts/{video_id}.json` â€“ list of words with `start`, `end`, `word`. |

### 1â€‘B  |Â 10â€‘Second Segmentation & Normalisation

- `segment_transcript.py` converts word list â†’ 10â€¯s buckets.
- Normalise text: lowerâ€‘case, strip punctuation, collapse whitespace.  **Silent buckets** (â‰¤â€¯2Â words) retain empty string but keep times.

```jsonc
{
  "video_id": "demo.mp4",
  "segments": [
    {"start": 0.0,   "end": 10.0,  "text": "introduction of the device ..."},
    {"start": 10.0,  "end": 20.0,  "text": ""},
    ...
  ]
}
```

### 1â€‘C  |Â Embedding

- `embed_text.py` loads CLIPÂ **ViTâ€‘B/32** text encoder (`open_clip`).
- Batched forward pass (â‰¤â€¯32Â segments/batch, CPU).
- Persist embeddings â†’ `phase3_db` via gRPC interface (see Phaseâ€¯3 API).

**Deliverables PhaseÂ 1**

| Deliverable             | Acceptance Criteria                                                                               |
| ----------------------- | ------------------------------------------------------------------------------------------------- |
| `extract_transcribe.py` | JSON transcript matches Whisper CLI output; unit test verifies â‰¥â€¯95â€¯% coverage for timing fields. |
| `segment_transcript.py` | Exactly âŒˆduration/10âŒ‰ segments with correct start/end; silent buckets kept.                       |
| `embed_text.py`         | Embeds N segments in â‰¤â€¯( Nâ€¯/â€¯8â€¯)â€¯s on dev laptop (benchmark).                                     |

---

## PhaseÂ 2Â â€“ Visual Frame Extraction & Embedding

### 2â€‘AÂ |Â Frame Sampling (10â€¯s Grid)

- `ffmpeg -ss {t} -i video.mp4 -frames:v 1 frames/{id}_{t}.jpg` for tÂ =Â 0,10,20, â€¦
- Metadata: `start = t`, `end = t + 10`.

### 2â€‘BÂ |Â Frame Embedding

- Use **same CLIP model** image encoder.
- Images resized to 224Ã—224, center crop.
- BatchÂ sizeÂ =Â 64.
- Store embedding +Â metadata.

**Deliverables PhaseÂ 2**

| Deliverable        | Criterion                                                 |
| ------------------ | --------------------------------------------------------- |
| `sample_frames.py` | All frames exist; naming matches regex `videoid_\d+.jpg`. |
| `embed_frames.py`  | Embedding file or direct DB push validated by checksum.   |

---

## PhaseÂ 3Â â€“ Vector Store Service (ChromaDB)

### 3â€‘AÂ |Â Containerised DB

- Docker service in `docker-compose.yml` â€“ Chroma with `CHROMA_DB_IMPL=duckdb+parquet`, volumeâ€‘mounted under `data/chroma/`.

### 3â€‘BÂ |Â Schema & API

One **collection** `video_segments` with schema:

```python
{
  "id": str,                    # UUID
  "embedding": List[float],
  "metadata": {
       "video_id": str,
       "modality": "audio"|"frame",
       "start": float,          # seconds
       "end": float,
       "path": str|null         # jpeg for frames
  }
}
```

*Expose a thin gRPC layer* (`src/phase3_db/server.py`) so Phaseâ€¯1 &Â 2 import `DbClient` without raw Chroma dependency.

### 3â€‘CÂ |Â Batch Ingestion

- `DbClient.add_batch(vectors, metadatas)` with **batch\_size=20**.
- Unit test: ingest 100 dummy vectors, assert count==100.

**Deliverables PhaseÂ 3**: Docker compose, `DbClient`, ingestion tests.

---

## PhaseÂ 4Â â€“ Retrieval Service

### 4â€‘AÂ |Â Query Embedding

- `embed_query.py` â†’ CLIP text encoder.

### 4â€‘BÂ |Â Search Endpoint

`Retriever.search(query: str, k:int=10) -> List[Document]`

- Executes single cosineâ€‘similarity search in Chroma.
- Returns **rankâ€‘ordered** list; each `Document` holds `text` (for audio) *or* `"<IMAGE_FRAME>"` placeholder plus metadata.

**Deliverables PhaseÂ 4**

- `src/phase4_retriever/retriever.py` with pydantic models and coverage tests.

---

## PhaseÂ 5Â â€“ LLM Generation Microâ€‘service (ChatGroq Integration)

### 5-AÂ |Â Prompt Template (LangChain + ChatGroq)

```python
from langchain_groq import ChatGroq
SYSTEM = "You are a helpful assistant ... always cite timestamp and video ID."  # System prompt template
DOC_PROMPT = "[{metadata[start]:.2f}-{metadata[end]:.2f}s | {metadata.video_id}] {page_content}"  # Document formatting
llm = ChatGroq(model="llama3-70b-8192", temperature=0, max_tokens=None, reasoning_format="parsed")  # Llama 70B model (8192-token context):contentReference[oaicite:0]{index=0}
QA_CHAIN = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=Retriever(...),
    chain_type="stuff",
    chain_type_kwargs={
        "prompt": SYSTEM,               # base prompt (system instructions + question)
        "document_prompt": DOC_PROMPT   # how each retrieved segment is injected
    }
)
```

*Using LangChainâ€™s prompt templating, we inject the metadata of retrieved video segments (formatted with timestamps) into the LLMâ€™s input. The **ChatGroq** LLM (a Llama2/Llama3-based model) is run with `temperature=0` for deterministic, fact-focused answers (minimizing hallucinations). The system prompt instructs the model to cite timestamps and video IDs, while each document chunk is formatted via `DOC_PROMPT` and automatically inserted into the final prompt.*

### 5-BÂ |Â API Wrapper

* Implement a FastAPI endpoint `/ask` that calls the `QA_CHAIN` and returns a JSON response: `{ "answer": str, "sources": List[metadata] }`. The answer string is generated by the ChatGroq LLM (with the above prompt template), and `sources` are the metadata objects for each referenced video segment (containing video\_id and timestamp range).
* Ensure the service handles ChatGroqâ€™s async API calls or streaming if applicable (use LangChainâ€™s integration which manages the Groq API key and call parameters). The endpoint should respond with the LLMâ€™s answer and source list within reasonable latency.

**Deliverables PhaseÂ 5**: Prompt template files, the `qa_service.py` micro-service (integrating ChatGroq via LangChain), and an end-to-end test using a sample video/query to verify the answer includes correct timestamps in the output. Moreover the integration of deliverables in 
the pipeline built uptil now

---

## PhaseÂ 6Â â€“ Clip Builder

### 6â€‘AÂ |Â Timestamp Parser

- Regexes for `HH:MM:SS`, `MM:SS`, `SS.S` â†’ seconds.
- Merge intervals with â‰¤â€¯2â€¯s gap.

### 6â€‘BÂ |Â Clip Extraction

- Use `moviepy.VideoFileClip(video).subclip(start,end).write_videofile(outfile, codec="libx264")`.
- Runs asynchronously (`asyncio`) so multiple clips export in parallel.

### 6â€‘CÂ |Â Clip Registry

- SQLite table `clips(id, video_id, start, end, path, created_at)` for reâ€‘use.

**Deliverables PhaseÂ 6**: `clipper.py`, integration test â€“ feed answer JSON, verify clips exist and duration matches.

---

## ðŸ“…Â Milestone Timeline & Team Allocation

| Week | Phase / Component     | Owner            | Key Review                   |
| ---- | --------------------- | ---------------- | ---------------------------- |
| 1    | PhaseÂ 1â€‘A/B           | BackendÂ 1        | Transcript JSON contract     |
| 2    | PhaseÂ 1â€‘C + PhaseÂ 2â€‘A | BackendÂ 1 / MLÂ 1 | Embedding fidelity test      |
| 3    | PhaseÂ 2â€‘B + PhaseÂ 3   | MLÂ 1 / DevOpsÂ 1  | DB ingestion benchmark       |
| 4    | PhaseÂ 4               | BackendÂ 2        | Retrieval precision\@10      |
| 5    | PhaseÂ 5               | MLÂ 2             | Answer quality & token costs |
| 6    | PhaseÂ 6               | BackendÂ 2        | Full e2e demo                |

---

## âœ…Â Readiness Checklist per Phase

-

---

## Future Enhancements (Postâ€‘MVP)

1. **UI**: React frontâ€‘end with embedded `<video>` + timeline markers.

---

Â©Â 2025Â KRÂ xÂ OPÂ DevÂ Team

